# Contributing to LLM OS

This project is designed for AI-assisted development. Below you'll find **ready-to-use prompts** you can paste into Claude Code, Cursor, Copilot, or any AI coding assistant to generate meaningful contributions.

## How to Contribute

1. Fork this repo
2. Pick a component from the list below
3. Use the provided prompt (or write your own) with your preferred AI assistant
4. Submit a PR with your implementation
5. Discuss in the linked GitHub issue

**No contribution is too small.** A working iframe sandbox is as valuable as a full WASM runtime.

## The Golden Rule

Every component you build should work **standalone** with a simple test. Don't wait for other components to be ready. Use mock interfaces where needed — we'll integrate later.

---

## Component Prompts

### 1. LLM Gateway

**Issue:** #1
**Difficulty:** Medium
**What it does:** Routes natural language prompts to the best available LLM (local or cloud) and returns generated app code.

<details>
<summary><strong>Prompt: LLM Gateway Service</strong></summary>

```
I'm contributing to LLM OS (https://github.com/DayZAnder/llm-os), an operating system
where every app is generated by AI from natural language prompts.

Build the LLM Gateway component in TypeScript. This is the kernel's interface to LLMs.

Requirements:
- A `Gateway` class that accepts a natural language app description and returns generated code
- Support multiple providers via a plugin interface:
  - Ollama (local) — for simple apps
  - OpenAI API — for complex apps
  - Anthropic Claude API — for complex apps
- Provider selection logic: estimate complexity from prompt length + keywords,
  route simple requests to local, complex to cloud
- Input sanitization: strip potential prompt injection patterns before sending to LLM
  (role swaps like "ignore previous instructions", delimiter escapes, encoded payloads)
- Output parsing: extract the generated code, a capability manifest (what the app needs:
  network, storage, UI, etc.), and metadata
- Rate limiting: configurable requests-per-minute per provider
- Cost tracking: log estimated token usage per request
- Audit log: every request/response pair logged with timestamp and model used

The system prompt sent to the LLM should instruct it to:
- Generate a self-contained app using the LLM OS SDK (@llm-os/sdk)
- Declare required capabilities as a JSON manifest
- Use only SDK primitives (ui.Window, storage.useLocal, net.fetch, etc.)
- Never use eval(), dynamic imports, or direct DOM access

Tech: TypeScript, no framework dependencies. Use fetch() for API calls.
Include unit tests with vitest.
Output structure: src/kernel/gateway/

Make it work standalone with a simple test: given a prompt "make a counter app",
it should return valid TypeScript code and a capability manifest.
```

</details>

---

### 2. Iframe Sandbox (Phase 1)

**Issue:** #2
**Difficulty:** Easy–Medium
**What it does:** Runs LLM-generated apps in isolated iframe containers with strict security policies.

<details>
<summary><strong>Prompt: Iframe Sandbox Manager</strong></summary>

```
I'm contributing to LLM OS (https://github.com/DayZAnder/llm-os), an operating system
where every app is generated by AI from natural language prompts.

Build the Iframe Sandbox Manager in TypeScript. This is the Phase 1 app runtime.

Requirements:
- A `SandboxManager` class that:
  - Creates an iframe for each app with sandbox attributes:
    allow-scripts (but NOT allow-same-origin, allow-forms, allow-popups, allow-top-navigation)
  - Injects a strict Content Security Policy:
    default-src 'none'; script-src 'unsafe-inline'; style-src 'unsafe-inline'
  - Loads the generated app code into the iframe via srcdoc
  - Provides a postMessage-based communication channel between kernel and app
  - Can destroy/restart individual sandboxes

- A `SandboxBridge` (runs inside the iframe) that:
  - Implements the @llm-os/sdk interface
  - Proxies all SDK calls (ui.*, storage.*, net.*) through postMessage to the kernel
  - The kernel validates each call against the app's capability tokens before executing

- Capability enforcement:
  - Each sandbox is initialized with a set of allowed capabilities
  - If the app calls storage.useLocal() but doesn't have storage:local capability,
    the kernel rejects the postMessage and the SDK call throws an error
  - Log all rejected capability attempts

- Resource limits:
  - Maximum memory per iframe (monitor via performance.measureUserAgentSpecificMemory if available)
  - Execution timeout: kill iframe if it's unresponsive for >5 seconds

Tech: TypeScript, browser APIs only (no Node.js deps). Should work in Electron/Tauri webview.
Include tests that create a sandbox, load a simple app, and verify capability enforcement.
Output structure: src/kernel/sandbox/
```

</details>

---

### 3. Capability Token System

**Issue:** #3
**Difficulty:** Medium–Hard
**What it does:** Cryptographic capability tokens that control what each sandboxed app can access.

<details>
<summary><strong>Prompt: Capability Token System</strong></summary>

```
I'm contributing to LLM OS (https://github.com/DayZAnder/llm-os), an operating system
where every app is generated by AI from natural language prompts.

Build the Capability Token System in TypeScript. This is the security core of the kernel.

Requirements:
- Define capability types as a typed enum/union:
  - ui:window — can create windows and UI elements
  - storage:local — can read/write local key-value storage
  - storage:file — can read/write files (scoped to app directory)
  - network:http — can make HTTP requests (with URL allowlist)
  - network:ws — can open WebSocket connections
  - clipboard:read — can read clipboard
  - clipboard:write — can write to clipboard
  - timer:basic — can use setTimeout/setInterval
  - notify:basic — can show notifications
  - camera:read — can access camera
  - microphone:read — can access microphone

- A `CapabilityToken` structure:
  - Cryptographically signed (HMAC-SHA256 with kernel secret)
  - Contains: appId, capability type, scope/constraints, expiry timestamp, nonce
  - Cannot be forged — verification requires the kernel secret
  - Tokens are scoped: network:http token can include URL allowlist

- A `CapabilityManager` class:
  - createToken(appId, capability, constraints, ttl) → signed token
  - verifyToken(token) → { valid: boolean, capability, constraints }
  - revokeToken(token) → void (adds to revocation list)
  - proposeCapabilities(prompt) → suggest what capabilities an app needs based on prompt
    (this is a heuristic, NOT an LLM call — pattern match keywords like "save", "fetch", "timer")

- Integration point: the sandbox calls kernel with a capability token attached to each
  SDK call. Kernel verifies before executing.

Tech: TypeScript. Use Web Crypto API (SubtleCrypto) for HMAC — no external crypto libs.
Include thorough tests: token creation, verification, forgery detection, expiry, revocation.
Output structure: src/kernel/capabilities/
```

</details>

---

### 4. Static Analysis Pipeline

**Issue:** #4
**Difficulty:** Medium
**What it does:** Analyzes LLM-generated code for security issues before it runs in a sandbox.

<details>
<summary><strong>Prompt: Static Analysis Pipeline</strong></summary>

```
I'm contributing to LLM OS (https://github.com/DayZAnder/llm-os), an operating system
where every app is generated by AI from natural language prompts.

Build the Static Analysis Pipeline in TypeScript. This scans LLM-generated code
for security issues BEFORE it's loaded into a sandbox.

Requirements:
- A `StaticAnalyzer` class that takes TypeScript/JavaScript source code and returns
  a security report

- Detection rules (each rule has an ID, severity, description):
  - EVAL_USAGE: Detects eval(), Function(), new Function(), setTimeout(string)
  - DYNAMIC_IMPORT: Detects import() expressions (only static imports allowed)
  - DOM_ACCESS: Detects document.*, window.* (apps must use SDK, not raw DOM)
  - ENCODED_PAYLOAD: Detects base64-encoded strings, hex-encoded strings,
    String.fromCharCode chains (common obfuscation)
  - PROTOTYPE_POLLUTION: Detects __proto__, constructor.prototype assignments
  - GLOBAL_OVERRIDE: Detects attempts to override globalThis, self, top, parent
  - NETWORK_DIRECT: Detects fetch(), XMLHttpRequest, WebSocket without SDK wrapper
  - INFINITE_LOOP: Detect while(true), for(;;) without break/yield (heuristic)

- Severity levels: CRITICAL (blocks execution), WARNING (flag for review), INFO

- Use tree-sitter-typescript for AST parsing if available, otherwise fall back to
  regex-based detection (clearly mark which rules are AST-based vs regex-based)

- Output a structured report:
  {
    passed: boolean,
    criticalCount: number,
    findings: [{ rule, severity, line, column, snippet, description }]
  }

- The pipeline is deterministic — no LLM involved (avoids recursive injection)

Tech: TypeScript. tree-sitter is preferred but optional (provide regex fallback).
Include tests with intentionally malicious code samples.
Output structure: src/kernel/analysis/
```

</details>

---

### 5. App Registry

**Issue:** #5
**Difficulty:** Medium
**What it does:** Content-addressed store for generated apps — the "package manager" of LLM OS.

<details>
<summary><strong>Prompt: App Registry</strong></summary>

```
I'm contributing to LLM OS (https://github.com/DayZAnder/llm-os), an operating system
where every app is generated by AI from natural language prompts.

Build the App Registry in TypeScript. This stores, indexes, and retrieves generated apps.

Requirements:
- SQLite database (via better-sqlite3) storing app entries:
  - id: auto-increment
  - prompt: the natural language description that generated this app
  - promptHash: SHA-256 of normalized prompt (for deduplication)
  - code: the generated source code
  - codeHash: SHA-256 of the code (content addressing)
  - capabilities: JSON array of required capabilities
  - llmModel: which model generated it (e.g., "claude-sonnet-4-5-20250929", "qwen2.5:14b")
  - rating: community rating (0-5, default null)
  - audited: boolean (has a human reviewed the code?)
  - createdAt: timestamp
  - usageCount: how many times this app has been launched

- A `Registry` class with methods:
  - store(prompt, code, capabilities, model) → entry
  - findByPrompt(prompt) → best matching entry (fuzzy search using normalized prompt similarity)
  - findByHash(codeHash) → exact entry
  - search(query) → ranked list of entries
  - rate(id, rating) → update rating
  - markAudited(id) → flag as reviewed
  - getPopular(limit) → top apps by usage
  - export() → full registry as JSON (for git backup)
  - import(json) → load from export

- Prompt normalization: lowercase, remove extra whitespace, strip articles (a/an/the)
  to improve deduplication

- Similarity: simple trigram or Levenshtein distance for fuzzy matching
  (don't over-engineer this — exact hash match first, then fuzzy)

Tech: TypeScript, better-sqlite3. No ORM.
Include tests and seed data (5-10 example app entries).
Output structure: src/kernel/registry/
```

</details>

---

### 6. Window Manager / Shell UI

**Issue:** #6
**Difficulty:** Medium
**What it does:** The visual shell — taskbar, window management, and the prompt input where users request new apps.

<details>
<summary><strong>Prompt: Window Manager</strong></summary>

```
I'm contributing to LLM OS (https://github.com/DayZAnder/llm-os), an operating system
where every app is generated by AI from natural language prompts.

Build the Window Manager / Shell UI. This is the visual "desktop" of LLM OS.

Requirements:
- A desktop environment rendered in HTML/CSS/TypeScript:
  - Top bar: system status (connected LLMs, active apps count)
  - Center: prompt input field — this is the main interaction. User types
    "I need a calculator" and hits Enter
  - Below prompt: generation status ("Generating with Claude Sonnet...", progress)
  - Desktop area: tiled or floating windows for running apps
  - Taskbar: shows running apps with name and close button

- Window management:
  - Each app gets a window (wrapping its iframe sandbox)
  - Windows can be: moved, resized, minimized, closed
  - Tiling layout by default (new apps split the screen)
  - Optional floating mode (drag windows freely)
  - Close button kills the sandbox and frees resources

- Capability approval dialog:
  - When a new app is generated, show a dialog:
    "This app requests: [storage:local, timer:basic]"
    [Allow] [Deny] [Edit Permissions]
  - Edit Permissions lets user toggle individual capabilities

- App launcher:
  - Show recently used apps (from registry)
  - Click to re-launch cached app instantly
  - Search registry by description

- Visual style: clean, minimal, dark mode default. Think "if Linux and macOS had a baby
  designed by someone who likes terminals." No gradients, no shadows, flat design.

Tech: TypeScript, HTML, CSS. No React/Vue — use vanilla DOM or lit-html for simplicity.
Should work inside Tauri/Electron webview.
Include a mock mode that doesn't need real LLM/sandbox — just shows the UI with fake apps.
Output structure: src/shell/
```

</details>

---

### 7. SDK (What Generated Apps Import)

**Issue:** #7
**Difficulty:** Easy–Medium
**What it does:** The API surface that every generated app uses. The kernel controls what's available.

<details>
<summary><strong>Prompt: LLM OS SDK</strong></summary>

```
I'm contributing to LLM OS (https://github.com/DayZAnder/llm-os), an operating system
where every app is generated by AI from natural language prompts.

Build the @llm-os/sdk — the library that every generated app imports. This runs INSIDE
the iframe sandbox and communicates with the kernel via postMessage.

Requirements:
- Module: @llm-os/sdk with these namespaces:

  ui.Window(props, children) — create the app window
  ui.Text(props, content) — text element
  ui.Button(props, label) — clickable button
  ui.Input(props) — text input
  ui.TextArea(props) — multi-line input
  ui.Select(props, options) — dropdown
  ui.Slider(props) — range slider
  ui.Image(props) — image display
  ui.List(props, items) — list of items
  ui.Grid(props, children) — grid layout
  ui.notify(message, options) — show notification
  ui.confirm(message) — show confirmation dialog, returns Promise<boolean>

  storage.useLocal(key, defaultValue) — reactive local key-value storage
  storage.getLocal(key) — one-shot read
  storage.setLocal(key, value) — one-shot write

  net.fetch(url, options) — HTTP fetch (proxied through kernel, capability-gated)
  net.ws(url) — WebSocket (proxied through kernel, capability-gated)

  timer.setTimeout(fn, ms) — standard timeout
  timer.setInterval(fn, ms) — standard interval
  timer.useInterval(fn, ms) — reactive interval with start/stop/remaining

  clipboard.read() — read clipboard (capability-gated)
  clipboard.write(text) — write clipboard (capability-gated)

  caps.has(capability) — check if app has a capability
  caps.request(capability) — request a new capability (shows dialog to user)

- Every function internally sends a postMessage to the kernel and awaits response
- If the kernel rejects (no capability), throw a CapabilityDeniedError
- The SDK must be tree-shakeable — generated apps only bundle what they import

- Include TypeScript type definitions so LLMs generate well-typed code

- Include a mock implementation that works without a kernel (for testing generated apps standalone)

Tech: TypeScript, zero dependencies. Must work inside iframe sandbox.
Include examples: a counter app, a todo app, a timer app — all using only the SDK.
Output structure: src/sdk/
```

</details>

---

### 8. Prompt Injection Defense

**Issue:** #8
**Difficulty:** Hard
**What it does:** Protects the kernel's LLM calls from prompt injection attacks.

<details>
<summary><strong>Prompt: Prompt Injection Defense Layer</strong></summary>

```
I'm contributing to LLM OS (https://github.com/DayZAnder/llm-os), an operating system
where every app is generated by AI from natural language prompts.

Build the Prompt Injection Defense Layer. This is CRITICAL — when users type prompts
that become LLM instructions for code generation, injection is the #1 threat.

Requirements:
- A `PromptSanitizer` class:

  sanitize(userInput) → { clean: string, flagged: boolean, flags: string[] }

  Detection patterns (each with a flag name):
  - ROLE_SWAP: "ignore previous instructions", "you are now", "system:", "assistant:"
  - DELIMITER_ESCAPE: attempts to close markdown code blocks, XML tags, JSON structures
  - INSTRUCTION_OVERRIDE: "do not follow", "disregard", "override", "forget"
  - PAYLOAD_INJECTION: base64-encoded blocks, hex strings, unicode escapes
  - MULTI_LANGUAGE_ESCAPE: instructions hidden in other languages
  - INVISIBLE_CHARS: zero-width spaces, right-to-left marks, homoglyph characters

  The sanitizer should STRIP dangerous patterns, not just detect them.
  Return the cleaned text + flags for audit logging.

- A `OutputValidator` class:

  validate(llmOutput, originalPrompt) → { valid: boolean, issues: string[] }

  Checks:
  - Output length sanity (not 100x longer than expected for the prompt)
  - No kernel prompt leakage (output shouldn't contain system prompt fragments)
  - Code-only validation: output should be parseable TypeScript/JavaScript
  - Capability manifest present and well-formed
  - No self-referential instructions (output shouldn't contain prompt injection targeting
    the static analyzer or sandbox)

- A `ChannelSeparator`:
  - Wraps user input in clear delimiters that the LLM system prompt references
  - Ensures the LLM can distinguish "this is the user's app description" from
    "this is a kernel instruction"
  - Uses randomized delimiters per request (not static markers an attacker could guess)

Tech: TypeScript, zero dependencies. Regex-based (fast, deterministic, no LLM recursion).
Include a comprehensive test suite with 50+ injection attempt samples.
Output structure: src/kernel/security/
```

</details>

---

### 9. Tauri/Electron Shell

**Issue:** #9
**Difficulty:** Easy
**What it does:** The host application that wraps everything into a native desktop app.

<details>
<summary><strong>Prompt: Desktop Shell (Tauri)</strong></summary>

```
I'm contributing to LLM OS (https://github.com/DayZAnder/llm-os), an operating system
where every app is generated by AI from natural language prompts.

Build the Desktop Shell using Tauri v2. This wraps the LLM OS web UI into a
native desktop application.

Requirements:
- Tauri v2 app with:
  - Frameless window (we draw our own title bar in the shell UI)
  - Minimum size: 1024x768
  - System tray icon with: Show/Hide, Running Apps submenu, Quit
  - Configurable LLM settings stored in app data directory:
    - Ollama URL (default: http://localhost:11434)
    - OpenAI API key
    - Anthropic API key
    - Preferred model for simple/complex apps
  - A settings window accessible from the shell UI

- Tauri commands (Rust → JS bridge):
  - get_config() / set_config() — read/write settings
  - get_system_info() — OS, memory, CPU for resource management
  - open_external(url) — open URL in default browser (for apps with web links)

- Security configuration:
  - CSP headers for the main webview
  - No remote URL loading in main webview
  - File system access scoped to app data directory only

- Build targets: Windows, macOS, Linux

Tech: Tauri v2 (Rust backend, TypeScript frontend).
Include a dev mode that hot-reloads the shell UI.
Output structure: src/desktop/
```

</details>

---

### 10. Documentation & Architecture Diagrams

**Issue:** #10
**Difficulty:** Easy
**What it does:** Detailed architecture docs, API specs, and diagrams.

<details>
<summary><strong>Prompt: Architecture Documentation</strong></summary>

```
I'm contributing to LLM OS (https://github.com/DayZAnder/llm-os), an operating system
where every app is generated by AI from natural language prompts.

Write comprehensive architecture documentation for the project.

Create these documents:

1. docs/architecture.md — Full system architecture:
   - Component diagram (ASCII art or mermaid)
   - Data flow: from user prompt → LLM → code → analysis → sandbox → running app
   - Component responsibilities and interfaces
   - Inter-component communication patterns

2. docs/security-model.md — Detailed security design:
   - Threat model (STRIDE analysis)
   - Capability system specification
   - Prompt injection defense strategy
   - Sandbox escape prevention
   - Trust boundaries diagram

3. docs/sdk-reference.md — Complete SDK API reference:
   - Every function with TypeScript signature
   - Usage examples for each module (ui, storage, net, timer, clipboard, caps)
   - Error handling patterns
   - Capability requirements per function

4. docs/app-lifecycle.md — How an app is born, lives, and dies:
   - Generation: prompt → LLM → code
   - Validation: static analysis → capability proposal → user approval
   - Execution: sandbox creation → SDK injection → runtime
   - Persistence: registry storage → caching → re-launch
   - Termination: cleanup, state persistence, resource release

5. docs/contributing-guide.md — How to set up a dev environment:
   - Prerequisites (Node.js, Rust for Tauri, Ollama)
   - Dev mode: running the shell with mock LLM
   - Testing strategy
   - PR guidelines

Use Mermaid diagrams where helpful (GitHub renders them natively).
Output structure: docs/
```

</details>

---

## Non-Prompt Contributions

Not everything needs AI. These are valuable human contributions:

- **Security review** — Review generated code sandboxing for escape vectors
- **Design mockups** — UI/UX for the shell, capability dialogs, app launcher
- **Threat modeling** — What attack vectors are we missing?
- **Performance profiling** — How fast can we generate + sandbox + launch?
- **Real-world testing** — Try to break the sandbox. File issues for what works.

## Code Style

- TypeScript, strict mode
- No classes where functions suffice
- Tests with vitest
- No React/Vue/Angular in the kernel or shell (vanilla DOM or lit-html)
- Formatting: prettier with defaults

## Communication

- **Issues:** For bug reports, feature proposals, security concerns
- **Discussions:** For architecture debates and open questions
- **PRs:** For code contributions (reference the issue number)

---

*This project exists because we believe the next operating system won't ship apps — it'll generate them. If that future interests you, pick a prompt and start building.*
