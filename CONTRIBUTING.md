# Contributing to LLM OS

This project is built with AI, by AI, for everyone. Every contribution — whether generated by Claude Code, Cursor, Copilot, or hand-written — is welcome.

## Core Values — Every Contribution Must Respect These

Before writing a single line, internalize these. PRs that violate them will be rejected.

**1. Protect the user first.** No telemetry. No tracking. No data exfiltration. No phoning home. Generated apps are sandboxed. Capabilities require explicit user approval. When in doubt, deny access. User privacy is non-negotiable.

**2. Empower the user.** Never add artificial limitations. The OS exists to serve the user, not to gatekeep what they can run. If a user wants to generate and run an app, let them — unless it harms others.

**3. Take a piece, leave a piece.** You benefit from this project? Give something back. Even a bug report counts. Code that is contributed must not damage or disrupt the core idea of this OS.

**4. Nothing is perfect.** Ship working code, improve later. Don't over-engineer. Don't block progress waiting for perfection. But never violate the core intent.

---

## Contributing with Claude Code (VS Code)

**This is the easiest way to contribute.** The repo includes a `CLAUDE.md` that automatically gives Claude Code full project context, architecture, and values.

### Quick Start

```bash
# 1. Fork and clone
gh repo fork DayZAnder/llm-os --clone
cd llm-os

# 2. Open in VS Code with Claude Code
code .

# 3. Tell Claude Code what to build (examples below)

# 4. Test your changes
node src/server.js

# 5. Submit a PR
gh pr create
```

When you open this repo in VS Code with Claude Code, it reads `CLAUDE.md` and understands:
- The architecture and how components connect
- The core values and security requirements
- What code exists and what's still needed
- The coding style (vanilla JS, no frameworks, dark theme)

### Prompts to Paste into Claude Code

Pick one, paste it into Claude Code, and let it work. Each prompt is self-contained and includes the values context.

---

### Prompt 1: WASM Sandbox (replace iframes)

```
Read the existing iframe sandbox in src/shell/sandbox.js and src/sdk/sdk.js.

Build a WebAssembly-based sandbox to replace (or sit alongside) the iframe approach.
Use Wasmtime or Extism as the WASM runtime.

Core value reminder: Protect the user first. The WASM sandbox must be MORE restrictive
than iframes, not less. No filesystem access, no network access, no capabilities
unless explicitly granted by the kernel with user approval.

Requirements:
- A WasmSandbox class with the same interface as the iframe SandboxManager
- launch(appId, code, capabilities, title) → runs app in WASM sandbox
- kill(appId) → terminates sandbox
- postMessage-equivalent communication channel between kernel and WASM app
- Capability enforcement: WASM app cannot call any host function without a valid capability
- Memory limits per sandbox (configurable, default 64MB)
- CPU time limits (kill after 30 seconds of continuous execution)

Put it in src/kernel/wasm-sandbox/. Include tests.
Make it work standalone — don't break the existing iframe sandbox.
```

---

### Prompt 2: Cryptographic Capability Tokens

```
Read the existing simple capability system in src/kernel/capabilities.js.

Replace the in-memory whitelist with cryptographic capability tokens.

Core value reminder: Protect the user first. Tokens must be unforgeable.
A generated app must NEVER be able to escalate its own privileges.
All capability grants require explicit user approval.

Requirements:
- HMAC-SHA256 signed tokens using Web Crypto API (SubtleCrypto) — no npm crypto libs
- Token contains: appId, capability type, scope constraints, expiry, nonce
- Kernel generates a random secret on startup (regenerated each session)
- verifyToken() is constant-time to prevent timing attacks
- Revocation list for tokens that should be invalidated early
- Backward compatible: existing capability checks still work

Update src/kernel/capabilities.js in place. Add tests that verify:
- Valid tokens are accepted
- Tampered tokens are rejected
- Expired tokens are rejected
- Revoked tokens are rejected
- Apps cannot forge tokens without the kernel secret
```

---

### ~~Prompt 3: App Registry~~ — DONE

> Implemented in `src/kernel/registry/store.js`. Content-addressed with SHA-256, trigram fuzzy search, community sync from GitHub. See the Browse Apps button in the shell.

**Next step:** Improve the registry — add ratings, install counts, featured apps, or a web-based registry browser.

---

### ~~Prompt 4: Bootable Linux Image~~ — DONE

> Implemented in `build/`. Alpine Linux VM with Docker + Node.js, outputs QCOW2 (Proxmox) and VHDX (Hyper-V). Download from [Releases](https://github.com/DayZAnder/llm-os/releases).

**Next step:** Add kiosk browser mode (Chromium auto-launching into the shell), OVA for VirtualBox, or CI builds via GitHub Actions.

---

### ~~Prompt 3: Persistent Storage Layer~~ — DONE

> Implemented in `src/kernel/storage.js`. Per-app JSON files in `data/apps/<appId>/`, 5MB quota, path traversal protection, debounced writes, export/import. 43 tests in `tests/storage.test.js`.

**Next step:** Add storage usage display in the capability approval dialog, or add storage migration/versioning.

---

### Prompt 4: CI Pipeline (GitHub Actions)

```
Set up GitHub Actions to automatically build and test LLM OS on every push/PR.

Core value reminder: Nothing is perfect — but CI catches regressions before
they reach users. Every PR should pass before merge.

Requirements:
- Run all tests (node --test tests/*.test.js) on push and PR
- Run the values-check workflow (already exists, wire it in)
- Build the VM image on tagged releases (v*)
- Upload QCOW2 and VHDX as release assets automatically
- Build matrix: Node.js 22 on Ubuntu
- Cache node_modules between runs
- Badge in README showing build status

Put workflows in .github/workflows/. The VM build needs Docker (privileged).
```

---

### Prompt 5: Improve the Shell UI

```
Read src/shell/index.html — the current desktop UI.

Improve it while keeping the core architecture (vanilla JS, no frameworks).

Core value reminder: Empower the user. The UI should be fast, responsive,
and get out of the user's way. The prompt bar is the primary interaction.

Requirements:
- Add drag-and-drop window resizing (currently only tiling)
- Add window minimize/maximize (currently only close)
- Add a "recently generated" sidebar showing last 10 apps from the log
- Add keyboard shortcut: Ctrl+Space to focus the prompt bar
- Add keyboard shortcut: Ctrl+W to close the focused app
- Add a settings panel (accessible via gear icon) for:
  - Ollama URL
  - API keys (masked input)
  - Default capabilities to auto-approve
- Improve the capability approval dialog:
  - Show a brief explanation of each capability
  - Remember user preferences ("always allow timer:basic")
- Keep the dark color scheme: bg #0d0d1a, accent #6c63ff
- Keep it vanilla JS — no React, no Vue

Edit src/shell/index.html in place. Test by running node src/server.js.
```

---

### Prompt 6: Security Hardening

```
Read all files in src/kernel/ and src/sdk/sdk.js.

Audit the security of the current prototype and fix any issues you find.

Core value reminder: Protect the user first. This is the most important
contribution you can make. A sandbox escape or prompt injection bypass
means a generated app could harm the user's system.

Look for and fix:
1. Sandbox escape vectors: Can an iframe app access the parent frame?
   Can it make network requests? Can it read cookies or localStorage
   from the host?
2. Prompt injection bypasses: Can a user craft a prompt that makes the
   LLM ignore its system prompt? Can it leak the system prompt?
3. Static analysis gaps: What malicious patterns does analyzer.js miss?
   Add detection rules for anything you find.
4. Capability bypass: Can an app call storage/network without having
   the capability approved?
5. postMessage security: Is the message origin validated? Can a third-party
   page inject messages?

For each issue found:
- Fix it in the code
- Add a test case that verifies the fix
- Add a comment explaining the attack vector

Put new tests in tests/security/. Create the directory if needed.
```

---

### Prompt 7: Rust Microkernel Research Spike

```
This is a research task, not production code.

Create a proof-of-concept Rust no_std microkernel that boots via UEFI
and renders "LLM OS" to a framebuffer.

Core value reminder: Nothing is perfect. This is an experiment. It won't
be complete. The goal is to prove we CAN boot without Linux and show the
path forward for Phase 4.

Requirements:
- Rust #![no_std] #![no_main]
- UEFI boot via uefi-rs crate
- Get the framebuffer from UEFI GOP (Graphics Output Protocol)
- Render "LLM OS v0.0.1" to the screen using a bitmap font
- Set up a basic heap allocator
- Bonus: initialize a simple serial console for debug output
- Bonus: basic keyboard input via UEFI SimpleTextInput

Put it in experimental/microkernel/. Include a README with build
instructions (cargo build --target x86_64-unknown-uefi).
This should boot in QEMU with OVMF firmware.
```

---

## Low-Token Contributions (Quick Wins)

Have limited AI tokens? These focused tasks are small, self-contained, and valuable. Each takes 5–15 minutes.

### Security Test Vectors

```
Read tests/security/injection-vectors.json and tests/security/analyzer-vectors.json.
Add 5 NEW test vectors to each file. Focus on edge cases the existing vectors miss:
- Unicode homoglyph attacks (Cyrillic а instead of Latin a)
- Multi-line comment tricks (/* */ wrapping malicious code)
- CSS @import data exfiltration
- WebRTC-based fingerprinting
Follow the existing JSON format exactly. Each vector needs: input, shouldFlag (boolean), category, description.
```

### Example App

```
Read the examples/ directory. Look at calculator.html and todo.html for the pattern.
Build ONE new example app (pick one: unit converter, color picker, markdown previewer, habit tracker).
Use the same style: self-contained HTML, dark theme (#1a1a2e bg, #6c63ff accent),
capabilities comment on line 1, LLMOS.storage for persistence if needed.
Keep it under 120 lines.
```

### Analyzer Rule

```
Read src/kernel/analyzer.js. Understand the rule format.
Add ONE new detection rule for a pattern the analyzer currently misses.
Ideas: WebRTC access, SharedArrayBuffer, Blob URL creation, CSS expression(),
document.write, innerHTML with user input, importScripts in workers.
Add a matching test vector to tests/security/analyzer-vectors.json.
```

### Documentation Fix

```
Read README.md and CONTRIBUTING.md. Find anything unclear, outdated, or missing.
Fix it. Examples: broken links, unclear setup steps, missing prerequisites,
typos, better explanations for non-native English speakers.
```

---

## How to Contribute (Any Tool, Any AI, or No AI)

You don't need VS Code or Claude Code. This project welcomes contributions from any tool, any AI, or plain hand-written code. Pick whatever workflow suits you.

### Claude Code CLI (Terminal)

Works on any terminal — no IDE required.

```bash
# Fork, clone, and start working
gh repo fork DayZAnder/llm-os --clone && cd llm-os

# Claude Code reads CLAUDE.md automatically — it already knows the project
claude

# Or give it a task directly
claude "Read src/kernel/analyzer.js and add a detection rule for WebRTC data channels"

# Run tests when done
node tests/analyzer.test.js
node tests/storage.test.js
node tests/registry.test.js
```

Claude Code CLI reads `CLAUDE.md` on startup just like the VS Code extension — full project context, architecture, values, and coding style are all loaded automatically.

### Cursor / Windsurf / Cline / Aider

These editors and agents also read `CLAUDE.md` (or their equivalent context files). Clone the repo, open it, and the AI has full context.

```bash
gh repo fork DayZAnder/llm-os --clone && cd llm-os

# Cursor
cursor .

# Windsurf
windsurf .

# Aider (terminal-based)
aider --read CLAUDE.md
```

Pick a prompt from the list above and paste it into your AI assistant's chat.

### ChatGPT / Gemini / Grok / any other AI

These don't auto-read project files, so prepend this context block to any prompt:

```
PROJECT CONTEXT:
I'm contributing to LLM OS (https://github.com/DayZAnder/llm-os), an operating
system where every app is generated by AI from natural language prompts.

Core values (non-negotiable):
1. PROTECT THE USER FIRST — privacy, no telemetry, sandbox everything, user approves all capabilities
2. EMPOWER THE USER — no artificial limits, let them run what they want
3. TAKE A PIECE, LEAVE A PIECE — open source, contribute back, don't break the core
4. NOTHING IS PERFECT — ship working code, iterate, never violate core intent

Tech: Node.js 22+, vanilla JS (no React/Vue), ES modules, dark theme (#0d0d1a).
Zero runtime dependencies. Tests run with `node tests/*.test.js`.
Security: iframe sandboxes, capability-based access, regex static analyzer, prompt injection defense.

Key files:
- src/kernel/analyzer.js — static security analysis (35 regex rules)
- src/kernel/storage.js — per-app persistent storage (JSON files, 5MB quota)
- src/kernel/capabilities.js — capability-based access control
- src/kernel/gateway.js — LLM integration (pluggable: Ollama, Claude, OpenAI, and more)
- src/kernel/registry/store.js — app registry with search
- src/server.js — HTTP API server
- src/shell/index.html — desktop UI (vanilla JS, single file)
- src/sdk/sdk.js — SDK injected into sandboxed apps

Now, here's my task:
```

Then paste the specific task prompt. Copy the AI's output into the right files, test, and PR.

### Local LLMs (Ollama, llama.cpp, LM Studio)

If you run local models, you can use the project's own scripts as a starting point:

```bash
# Generate example apps with your local Ollama
node scripts/generate-apps.mjs

# Generate adversarial security test vectors
node scripts/generate-attack-vectors.mjs
```

These scripts use the Ollama API at `http://localhost:11434` by default (configurable via `OLLAMA_URL` env var). Any model that can generate HTML works — Qwen 2.5 14B, Llama 3, Mistral, etc.

### Contributing without AI

Human contributions are equally valuable — some of the most impactful work doesn't involve writing code at all:

- **Security audits** — Try to break the sandbox. Try prompt injection. File issues.
- **Threat modeling** — What attack vectors are we missing?
- **Design mockups** — UI/UX for the shell, capability dialogs, app launcher
- **Performance profiling** — Measure generation → sandbox → launch latency
- **Documentation** — Explain the architecture, write tutorials
- **Translations** — Localize the shell UI
- **Testing** — Run `node tests/*.test.js`, try edge cases, report failures

## Adding a Provider

The LLM gateway supports pluggable providers. To add a new one (e.g., Gemini, Mistral, a custom API):

**1. Create `src/kernel/providers/your-provider.js`:**

```javascript
export const provider = {
  name: 'your-provider',

  isAvailable(providerConfig) {
    return !!providerConfig.apiKey;
  },

  async checkHealth(providerConfig) {
    return !!providerConfig.apiKey;
  },

  async generate(messages, providerConfig, options = {}) {
    // messages: [{ role: 'system'|'user', content: string }]
    // options: { temperature, maxTokens }
    const res = await fetch('https://api.example.com/v1/generate', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${providerConfig.apiKey}`,
      },
      body: JSON.stringify({
        model: providerConfig.model,
        messages,
        temperature: options.temperature ?? 0.4,
        max_tokens: options.maxTokens ?? 4096,
      }),
    });
    if (!res.ok) throw new Error(`Provider error: ${res.status} ${await res.text()}`);
    const data = await res.json();
    return data.output; // Return the generated text string
  },
};
```

**2. Register it in `src/kernel/gateway.js`:**

```javascript
import { provider as yourProvider } from './providers/your-provider.js';
providers.set('your-provider', yourProvider);
```

**3. Add config in `src/kernel/config.js`:**

```javascript
providers: {
  // ... existing providers
  'your-provider': {
    apiKey: process.env.YOUR_PROVIDER_API_KEY || '',
    model: process.env.YOUR_PROVIDER_MODEL || 'default-model',
  },
},
```

**4. Add env vars to `.env.example`** and test with `node tests/gateway.test.js`.

If your provider uses the OpenAI-compatible `/v1/chat/completions` format (OpenRouter, Together, Groq, vLLM, LM Studio), you don't need a new provider at all — just set `OPENAI_BASE_URL` and `OPENAI_API_KEY`.

---

## Code Guidelines

- Vanilla JavaScript (ES modules, `type: "module"` in package.json)
- No classes where functions suffice
- No frameworks (React, Vue, Angular) in the shell or kernel
- Zero runtime dependencies unless absolutely necessary (currently: none)
- Tests welcome (vitest preferred, but any test runner works)
- Dark color scheme: `--bg-primary: #0d0d1a`, `--accent: #6c63ff`
- Every security decision should err on the side of denying access

## PR Checklist

Before submitting, verify:

- [ ] Does not add telemetry, analytics, or tracking of any kind
- [ ] Does not weaken sandbox isolation
- [ ] Does not bypass capability checks
- [ ] Does not add unnecessary dependencies
- [ ] Works standalone (can be tested independently)
- [ ] Follows the dark minimal visual style
- [ ] Includes tests for security-critical code
- [ ] References the relevant GitHub issue

## Communication

- **Issues:** Bug reports, feature proposals, security concerns
- **Discussions:** Architecture debates, open questions, ideas
- **PRs:** Code contributions (reference the issue number)
- **Security vulnerabilities:** File a private security advisory on GitHub

---

*This OS is built with AI, by AI, for humans. The next operating system won't ship apps — it will generate them. If that future interests you, clone the repo, pick your favorite tool, and start building.*
