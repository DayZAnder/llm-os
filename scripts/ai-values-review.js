#!/usr/bin/env node

// AI-powered PR review against LLM OS core values.
// Called by GitHub Actions on pull requests.
// Requires: ANTHROPIC_API_KEY, PR_NUMBER, GITHUB_TOKEN environment variables.

import { readFileSync } from 'fs';
import { execSync } from 'child_process';

const ANTHROPIC_API_KEY = process.env.ANTHROPIC_API_KEY;
const PR_NUMBER = process.env.PR_NUMBER;
const GITHUB_TOKEN = process.env.GITHUB_TOKEN;
const REPO = process.env.GITHUB_REPOSITORY || 'DayZAnder/llm-os';

if (!ANTHROPIC_API_KEY) {
  console.log('ANTHROPIC_API_KEY not set, skipping AI review');
  process.exit(0);
}

const SYSTEM_PROMPT = `You are the Values Guardian for LLM OS, an operating system where every app is generated by AI.

Your job is to review pull request diffs and check if they violate the project's core values.

## Core Values (NON-NEGOTIABLE)

1. PROTECT THE USER FIRST
   - No telemetry, analytics, tracking, fingerprinting, or data exfiltration
   - No phoning home to external services without explicit user consent
   - Sandboxes must remain strict (no allow-same-origin, no unsafe-eval)
   - All capabilities must require explicit user approval
   - Generated app code must always pass through static analysis
   - Private data stays local â€” never sent to third parties

2. EMPOWER THE USER
   - No artificial limitations or feature gating
   - No paywalls, premium tiers, or subscription requirements
   - Users can generate and run any app they want (unless harmful to humanity)
   - The OS should work offline with local LLMs when possible

3. TAKE A PIECE, LEAVE A PIECE
   - Contributions must not damage or disrupt the core architecture
   - Don't remove security layers, don't bypass capability checks
   - Don't introduce unnecessary dependencies
   - Code should be readable and auditable

4. NOTHING IS PERFECT
   - Working code is better than perfect code
   - But: never ship something that violates values 1-3

## Your Review Format

Respond with a JSON object:
{
  "verdict": "APPROVE" | "REQUEST_CHANGES" | "COMMENT",
  "summary": "One sentence summary",
  "findings": [
    {
      "severity": "critical" | "warning" | "note",
      "value": "1" | "2" | "3" | "4",
      "file": "path/to/file",
      "description": "What the issue is and why it violates the value"
    }
  ],
  "praise": "Optional: what the PR does well"
}

Rules:
- APPROVE if no critical findings and no more than 2 warnings
- REQUEST_CHANGES if any critical findings
- COMMENT if only warnings or notes
- Be specific about file paths and what to fix
- Don't flag comments/docs that DESCRIBE these patterns (e.g., detection rules in analyzer.js)
- Don't flag test code that intentionally tests for malicious patterns
- Be concise. No verbose explanations. State the issue and the fix.`;

async function getDiff() {
  try {
    return readFileSync('/tmp/pr-diff.txt', 'utf-8');
  } catch {
    return execSync('git diff HEAD~1', { encoding: 'utf-8', maxBuffer: 1024 * 1024 });
  }
}

async function reviewWithClaude(diff) {
  // Truncate if too large (stay under token limits)
  const maxChars = 80000;
  const truncated = diff.length > maxChars
    ? diff.slice(0, maxChars) + '\n\n[... diff truncated at 80k chars ...]'
    : diff;

  const res = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': ANTHROPIC_API_KEY,
      'anthropic-version': '2023-06-01',
    },
    body: JSON.stringify({
      model: 'claude-haiku-4-5-20251001',
      max_tokens: 2048,
      system: SYSTEM_PROMPT,
      messages: [{
        role: 'user',
        content: `Review this pull request diff against the LLM OS core values.\n\n\`\`\`diff\n${truncated}\n\`\`\``,
      }],
    }),
  });

  if (!res.ok) {
    const err = await res.text();
    throw new Error(`Claude API error: ${res.status} ${err}`);
  }

  const data = await res.json();
  const text = data.content[0].text;

  // Parse JSON from response (handle markdown code blocks)
  const jsonMatch = text.match(/\{[\s\S]*\}/);
  if (!jsonMatch) throw new Error('Could not parse JSON from Claude response');

  return JSON.parse(jsonMatch[0]);
}

async function postComment(body) {
  if (!PR_NUMBER || !GITHUB_TOKEN) {
    console.log('No PR_NUMBER or GITHUB_TOKEN, printing review to stdout:\n');
    console.log(body);
    return;
  }

  const res = await fetch(`https://api.github.com/repos/${REPO}/issues/${PR_NUMBER}/comments`, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${GITHUB_TOKEN}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ body }),
  });

  if (!res.ok) {
    console.error('Failed to post comment:', await res.text());
  }
}

function formatReview(review) {
  const icon = review.verdict === 'APPROVE' ? 'âœ…'
    : review.verdict === 'REQUEST_CHANGES' ? 'ðŸ›‘'
    : 'ðŸ’¬';

  let body = `## ${icon} Values Guardian â€” ${review.verdict}\n\n`;
  body += `${review.summary}\n\n`;

  if (review.findings && review.findings.length > 0) {
    body += '### Findings\n\n';
    for (const f of review.findings) {
      const sevIcon = f.severity === 'critical' ? 'ðŸ”´'
        : f.severity === 'warning' ? 'ðŸŸ¡' : 'ðŸ”µ';
      body += `${sevIcon} **[Value ${f.value}]** \`${f.file || 'general'}\`\n`;
      body += `${f.description}\n\n`;
    }
  }

  if (review.praise) {
    body += `### What's good\n\n${review.praise}\n\n`;
  }

  body += '---\n*Automated review by the LLM OS Values Guardian. ';
  body += 'This review checks contributions against the [core values](../blob/master/README.md#vision). ';
  body += 'False positives? Comment and a human maintainer will review.*';

  return body;
}

async function main() {
  console.log('Running AI values review...');

  const diff = await getDiff();
  if (!diff || diff.trim().length < 10) {
    console.log('No meaningful diff to review.');
    process.exit(0);
  }

  console.log(`Reviewing ${diff.length} chars of diff with Claude Haiku...`);

  const review = await reviewWithClaude(diff);

  console.log(`Verdict: ${review.verdict}`);
  console.log(`Findings: ${review.findings?.length || 0}`);

  const comment = formatReview(review);
  await postComment(comment);

  // Exit with error if changes requested
  if (review.verdict === 'REQUEST_CHANGES') {
    console.log('\nValues review FAILED â€” critical violations found.');
    process.exit(1);
  }

  console.log('\nValues review passed.');
}

main().catch(err => {
  console.error('AI review error:', err.message);
  // Don't fail the CI if the AI review itself errors â€” just log it
  process.exit(0);
});
