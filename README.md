# LLM OS

**An operating system where every application is generated by AI from natural language prompts.**

No pre-built apps. No compiled binaries. You describe what you need, and the OS generates it, sandboxes it, and runs it — with capability-based security ensuring generated code can only do what you approve.

> **Status:** Architecture & design phase. Looking for contributors.

## Why This Doesn't Exist Yet

The "LLM OS" label has been claimed by dozens of projects — agent frameworks, chatbot UIs, automation tools. None of them are actually an OS. Here's what exists vs. what we're building:

| Project | What It Actually Is | What's Missing |
|---------|-------------------|----------------|
| [AIOS](https://github.com/agiresearch/AIOS) (5.1k stars) | Agent scheduler/runtime | Doesn't generate apps — manages pre-built agents |
| [Open Interpreter](https://github.com/OpenInterpreter/open-interpreter) (62k stars) | CLI that runs LLM-generated code | No sandboxing, no capabilities, no persistence |
| Karpathy's "LLM OS" | Conceptual analogy from a talk | No implementation exists |
| Bolt.new, Replit, v0 | AI code generators | Generate code you deploy traditionally — not a runtime |
| E2B, Modal | Cloud sandbox-as-a-service | Infrastructure, not an OS |
| Claude Computer Use | Vision-based desktop automation | Controls existing apps, doesn't generate new ones |

**The gap:** A system where the LLM generates the application, the kernel sandboxes it, capability tokens control what it can access, and the prompt IS the source code.

## Architecture

```
┌──────────────────────────────────────────────┐
│  Generated Apps (sandboxed WASM / containers) │
├──────────────────────────────────────────────┤
│  App Runtime & Window Manager                 │
├──────────────────────────────────────────────┤
│  LLM OS Kernel                                │
│  ┌──────────┬──────────┬──────────┬────────┐  │
│  │ LLM      │ Sandbox  │ Cap-     │ App    │  │
│  │ Gateway  │ Manager  │ Based    │ Regis- │  │
│  │          │          │ Security │ try    │  │
│  └──────────┴──────────┴──────────┴────────┘  │
├──────────────────────────────────────────────┤
│  Host OS (Linux / macOS / Windows)            │
└──────────────────────────────────────────────┘
```

### Core Components

#### 1. LLM Gateway
The single chokepoint for all AI generation. Routes requests to the best available model:

- **Local models** (Ollama, llama.cpp) — simple UI tools, text processing
- **Cloud APIs** (Claude, GPT, Gemini) — complex applications, multi-file apps
- Enforces rate limits, cost budgets, audit logging
- Sanitizes all inputs/outputs against prompt injection

#### 2. Sandbox Manager
Every generated app runs in complete isolation:

- **Phase 1:** iframe sandboxes with strict CSP (web-based apps)
- **Phase 2:** WebAssembly (WASM) for near-native performance
- **Phase 3:** Firecracker/gVisor microVMs for apps needing syscalls

No app can touch the filesystem, network, or other apps without explicit capabilities.

#### 3. Capability-Based Security
Apps don't get "permissions" — they get cryptographic **capability tokens**:

```
User: "make me a todo app"
→ Kernel grants: [storage:local, ui:window]

User: "make me an email client"
→ Kernel grants: [network:smtp+imap, storage:local, ui:window, contacts:read]
```

- LLM proposes required capabilities based on the prompt
- User reviews and approves before app launches
- Generated code literally cannot call APIs without valid tokens
- Tokens are scoped, time-limited, and revocable

#### 4. App Registry
A content-addressed store for generated apps:

```json
{
  "prompt": "A markdown editor with live preview and vim keybindings",
  "generatedCode": "...",
  "codeHash": "sha256:a1b2c3...",
  "llmModel": "claude-sonnet-4-5-20250929",
  "capabilities": ["ui:window", "storage:local", "clipboard:read-write"],
  "rating": 4.7,
  "audited": true,
  "generatedAt": "2026-02-17T12:00:00Z"
}
```

- The **prompt is the source code** — regenerate anytime with a better model
- Git-backed, content-addressed (like Nix)
- Community can audit, rate, and fork prompts
- Cached apps load instantly; novel prompts generate fresh

### App Generation Flow

```
User types: "I need a pomodoro timer with break reminders"
                    │
                    ▼
   ┌─── Registry Lookup ───┐
   │ Similar app exists?    │
   │ Yes → offer cached     │──→ User accepts → launch sandbox
   │ No  → generate fresh   │
   └────────────────────────┘
                    │
                    ▼
   ┌─── LLM Generation ────┐
   │ Route to best model    │
   │ System prompt: LLM OS  │
   │ SDK + constraints      │
   │ Output: app code +     │
   │ capability manifest    │
   └────────────────────────┘
                    │
                    ▼
   ┌─── Security Gate ─────┐
   │ AST static analysis    │
   │ (no eval, no escapes)  │
   │ Capability review      │
   │ User approval dialog   │
   └────────────────────────┘
                    │
                    ▼
   ┌─── Launch ────────────┐
   │ Compile to sandbox     │
   │ Inject capability      │
   │ tokens + SDK           │
   │ Mount in window mgr    │
   └────────────────────────┘
```

### The SDK (What Generated Apps See)

```typescript
// Every generated app imports from the kernel SDK
import { ui, storage, net, timer, caps } from '@llm-os/sdk';

// Kernel injects ONLY the capabilities the app was granted
export default function App() {
  const [minutes, setMinutes] = storage.useLocal('minutes', 25);
  const alarm = timer.useInterval(() => {
    ui.notify('Break time!', { sound: true });
  }, minutes * 60 * 1000);

  return ui.Window({ title: 'Pomodoro Timer' }, [
    ui.Text({ size: 'xl' }, alarm.remaining),
    ui.Button({ onClick: alarm.toggle }, alarm.running ? 'Pause' : 'Start'),
    ui.Slider({ value: minutes, onChange: setMinutes, min: 1, max: 60 })
  ]);
}
```

If the app tries to call `net.fetch()` without a network capability token, the kernel throws — the function literally doesn't exist in its sandbox.

## Security Model

Prompt injection is the #1 threat when LLMs generate executable code.

### Three Layers of Defense

**Layer 1: Channel Separation**
- System instructions (kernel prompts) and user data (app content) travel on completely separate channels
- Generated apps never see kernel prompts
- Like kernel space vs. user space — but for prompts

**Layer 2: Static Analysis (No LLM in the Loop)**
- Every generated app goes through AST parsing before execution
- Detect: `eval()`, dynamic imports, capability escalation, encoded payloads
- This is deterministic code analysis — no recursive LLM vulnerability

**Layer 3: Runtime Containment**
- WASM sandbox means even if code is malicious, it's contained
- Capability tokens are cryptographic — cannot be forged
- All inter-app communication goes through a kernel message bus
- Resource limits (CPU time, memory) per app

### Threat Model

| Threat | Mitigation |
|--------|-----------|
| Prompt injection in user input | Sanitize before passing to LLM; separate instruction/data channels |
| Generated code with backdoor | AST static analysis + sandboxing + capability enforcement |
| Capability escalation | Tokens are cryptographic, scoped, kernel-enforced |
| Data exfiltration | Network capability required; all traffic logged |
| Cross-app attacks | Complete isolation; message bus is the only channel |
| Supply chain (malicious registry app) | Content-addressed, community-audited, signed by generating model |

## Tech Stack (Planned)

| Component | Technology | Why |
|-----------|-----------|-----|
| Shell / Window Manager | Tauri or Electron | Cross-platform desktop, web rendering |
| App Sandboxing (Phase 1) | iframe + strict CSP | Fast to prototype, well-understood |
| App Sandboxing (Phase 2) | WebAssembly (Extism/Wasmtime) | Memory-safe, near-native, portable |
| Local LLM | Ollama / llama.cpp | Free, private, fast for simple apps |
| Cloud LLM | Claude API, OpenAI API | Complex app generation |
| Registry Backend | SQLite + Git (content-addressed) | Simple, proven, works offline |
| Inter-Process Comm | postMessage → shared-nothing message bus | Secure by default |
| Static Analysis | tree-sitter + custom rules | Fast AST parsing, language-agnostic |

## Open Questions

These are genuinely unsolved and need community input:

1. **App composition** — Can one generated app call another? What's the stable ABI?
2. **State migration** — When you re-generate an app with a better model, how does data migrate?
3. **Non-determinism** — Same prompt + different model = different app. How do we version this?
4. **Performance** — LLM-generated code is rarely optimal. Auto-profile and suggest regeneration?
5. **Trust bootstrapping** — The kernel can't be LLM-generated (chicken-and-egg). Where's the trust root?
6. **Offline-first** — Should the OS work fully offline with local models, or require cloud?
7. **Multi-language** — Should generated apps be restricted to one language (TS?) or support many?

## Roadmap

### Phase 1: Proof of Concept
- [ ] Tauri/Electron shell with basic window manager
- [ ] iframe sandbox with strict CSP
- [ ] LLM gateway (Ollama + one cloud API)
- [ ] Simple capability system (network, storage, UI)
- [ ] Generate and run a "hello world" app from a prompt

### Phase 2: Core Kernel
- [ ] WASM-based sandbox (replace iframes)
- [ ] Cryptographic capability tokens
- [ ] AST-based static analysis pipeline
- [ ] App registry with content addressing
- [ ] Inter-app message bus

### Phase 3: Ecosystem
- [ ] Community app registry (GitHub-backed)
- [ ] App rating and security auditing
- [ ] Multi-model routing (local + cloud)
- [ ] App composition protocol
- [ ] State migration between app versions

### Phase 4: Real OS
- [ ] Custom bootloader or minimal Linux base
- [ ] Hardware abstraction layer (LLM-generated drivers?)
- [ ] Multi-user support
- [ ] Resource scheduling

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed instructions, including **ready-to-use AI prompts** you can paste into Claude, GPT, or your preferred coding assistant to generate contributions.

We've designed this project so that **AI-assisted contributions are first-class citizens**. Every component has a corresponding prompt that describes exactly what to build.

## Prior Art & Inspiration

- [Andrej Karpathy's LLM OS concept](https://www.youtube.com/watch?v=zjkBMFhNj_g) — the analogy that started the conversation
- [AIOS](https://github.com/agiresearch/AIOS) — agent scheduling research
- [E2B](https://e2b.dev/) — Firecracker-based sandboxing for AI code
- [Capability-based security](https://en.wikipedia.org/wiki/Capability-based_security) — the OS security model we're adopting
- [Nix](https://nixos.org/) — content-addressed, reproducible package management
- [WASI](https://wasi.dev/) — WebAssembly System Interface for sandboxed system access
- [OWASP Top 10 for Agentic AI (2026)](https://owasp.org/www-project-top-10-for-large-language-model-applications/) — security threat model

## License

MIT — see [LICENSE](LICENSE).
