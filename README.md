# LLM OS

**An operating system where every application is generated by AI from natural language prompts.**

No pre-built apps. No compiled binaries. You describe what you need, and the OS generates it, sandboxes it, and runs it — with capability-based security ensuring generated code can only do what you approve.

> **Status:** Architecture & design phase. Looking for contributors.

## Vision

One of the core principles of this project is **consensus of agents** — a group of AI agents that agree on what features are allowed to make it in. When there's doubt, a human maintainer decides (until that right is handed over to someone or something else).

### Core Rules

**1. Protect the user first.**
Their privacy and their integrity, through all means possible. This is non-negotiable.

**2. Empower the user.**
The OS should allow users to efficiently perform their tasks and launch any software or technically possible function they wish — as long as it is not detrimental to humanity as a whole. Users should be able to launch AI-generated software or, if they prefer, run other platforms. The tools should exist to do so efficiently.

**3. Take a piece, leave a piece.**
Every user may freely use this software on their own computer, network, or servers, and adapt it as they see fit. But if you benefit from it — contribute, even if just a little bit. Code that is contributed should not damage or otherwise disrupt the core idea of this OS.

**4. Nothing is perfect.**
These rules aren't perfect, but we can always improve — as long as the core intent isn't violated.

### How the Values Are Enforced

These aren't just words — they're enforced at every layer of the contribution pipeline:

```
Contributor writes code
        │
        ▼
┌─ Local Check ─────────────────────────────────┐
│ node scripts/values-check.js                   │
│ Deterministic scan: telemetry, sandbox          │
│ weakening, privacy violations, tracking         │
│ CRITICAL = blocked, WARNING = flagged          │
└────────────────────────────────────────────────┘
        │
        ▼
┌─ PR Self-Certification ───────────────────────┐
│ PR template: contributor checks each value     │
│ "No telemetry added" ✓  "No sandbox weakened" ✓│
└────────────────────────────────────────────────┘
        │
        ▼
┌─ CI: Deterministic Scan ─────────────────────┐
│ GitHub Actions runs values-check.js           │
│ Blocks merge on CRITICAL findings             │
└────────────────────────────────────────────────┘
        │
        ▼
┌─ CI: AI Values Guardian ─────────────────────┐
│ Claude reviews the diff against core values   │
│ Posts findings as PR comment                   │
│ Catches subtle violations regex can't see     │
└────────────────────────────────────────────────┘
        │
        ▼
┌─ Human Review ───────────────────────────────┐
│ Maintainer reviews with full context          │
│ Final authority on edge cases                 │
└────────────────────────────────────────────────┘
```

Three layers: deterministic checks (fast, always runs), AI review (catches nuance), human review (final authority). No single layer is trusted alone — they reinforce each other.

## Why This Doesn't Exist Yet

The "LLM OS" label has been claimed by dozens of projects — agent frameworks, chatbot UIs, automation tools. None of them are actually an OS. Here's what exists vs. what we're building:

| Project | What It Actually Is | What's Missing |
|---------|-------------------|----------------|
| [AIOS](https://github.com/agiresearch/AIOS) (5.1k stars) | Agent scheduler/runtime | Doesn't generate apps — manages pre-built agents |
| [Open Interpreter](https://github.com/OpenInterpreter/open-interpreter) (62k stars) | CLI that runs LLM-generated code | No sandboxing, no capabilities, no persistence |
| Karpathy's "LLM OS" | Conceptual analogy from a talk | No implementation exists |
| Bolt.new, Replit, v0 | AI code generators | Generate code you deploy traditionally — not a runtime |
| E2B, Modal | Cloud sandbox-as-a-service | Infrastructure, not an OS |
| Claude Computer Use | Vision-based desktop automation | Controls existing apps, doesn't generate new ones |

**The gap:** A system where the LLM generates the application, the kernel sandboxes it, capability tokens control what it can access, and the prompt IS the source code.

## Architecture

```
┌──────────────────────────────────────────────┐
│  Generated Apps (sandboxed WASM / containers) │
├──────────────────────────────────────────────┤
│  App Runtime & Window Manager                 │
├──────────────────────────────────────────────┤
│  LLM OS Kernel                                │
│  ┌──────────┬──────────┬──────────┬────────┐  │
│  │ LLM      │ Sandbox  │ Cap-     │ App    │  │
│  │ Gateway  │ Manager  │ Based    │ Regis- │  │
│  │          │          │ Security │ try    │  │
│  └──────────┴──────────┴──────────┴────────┘  │
├──────────────────────────────────────────────┤
│  Host OS (Linux / macOS / Windows)            │
└──────────────────────────────────────────────┘
```

### Core Components

#### 1. LLM Gateway
The single chokepoint for all AI generation. Pluggable provider registry routes requests to the best available model:

- **Local models** (Ollama) — simple UI tools, text processing
- **Cloud APIs** (Claude, OpenAI, OpenRouter, Together, Groq) — complex applications
- **Any OpenAI-compatible API** — vLLM, LM Studio, and custom endpoints via `OPENAI_BASE_URL`
- Configurable routing: set `PRIMARY_PROVIDER` / `FALLBACK_PROVIDER` or let auto-detection choose
- Sanitizes all inputs/outputs against prompt injection
- Adding a new provider is ~30 lines of code (see [CONTRIBUTING.md](CONTRIBUTING.md#adding-a-provider))

#### 2. Sandbox Manager
Every generated app runs in complete isolation:

- **Phase 1:** iframe sandboxes with strict CSP (web-based apps)
- **Phase 2:** WebAssembly (WASM) for near-native performance
- **Phase 3:** Firecracker/gVisor microVMs for apps needing syscalls

No app can touch the filesystem, network, or other apps without explicit capabilities.

#### 3. Capability-Based Security
Apps don't get "permissions" — they get cryptographic **capability tokens**:

```
User: "make me a todo app"
→ Kernel grants: [storage:local, ui:window]

User: "make me an email client"
→ Kernel grants: [network:smtp+imap, storage:local, ui:window, contacts:read]
```

- LLM proposes required capabilities based on the prompt
- User reviews and approves before app launches
- Generated code literally cannot call APIs without valid tokens
- Tokens are scoped, time-limited, and revocable

#### 4. App Registry
A content-addressed store for generated apps:

```json
{
  "prompt": "A markdown editor with live preview and vim keybindings",
  "generatedCode": "...",
  "codeHash": "sha256:a1b2c3...",
  "llmModel": "claude-sonnet-4-5-20250929",
  "capabilities": ["ui:window", "storage:local", "clipboard:read-write"],
  "rating": 4.7,
  "audited": true,
  "generatedAt": "2026-02-17T12:00:00Z"
}
```

- The **prompt is the source code** — regenerate anytime with a better model
- Git-backed, content-addressed (like Nix)
- Community can audit, rate, and fork prompts
- Cached apps load instantly; novel prompts generate fresh

### App Generation Flow

```
User types: "I need a pomodoro timer with break reminders"
                    │
                    ▼
   ┌─── Registry Lookup ───┐
   │ Similar app exists?    │
   │ Yes → offer cached     │──→ User accepts → launch sandbox
   │ No  → generate fresh   │
   └────────────────────────┘
                    │
                    ▼
   ┌─── LLM Generation ────┐
   │ Route to best model    │
   │ System prompt: LLM OS  │
   │ SDK + constraints      │
   │ Output: app code +     │
   │ capability manifest    │
   └────────────────────────┘
                    │
                    ▼
   ┌─── Security Gate ─────┐
   │ AST static analysis    │
   │ (no eval, no escapes)  │
   │ Capability review      │
   │ User approval dialog   │
   └────────────────────────┘
                    │
                    ▼
   ┌─── Launch ────────────┐
   │ Compile to sandbox     │
   │ Inject capability      │
   │ tokens + SDK           │
   │ Mount in window mgr    │
   └────────────────────────┘
```

### The SDK (What Generated Apps See)

```typescript
// Every generated app imports from the kernel SDK
import { ui, storage, net, timer, caps } from '@llm-os/sdk';

// Kernel injects ONLY the capabilities the app was granted
export default function App() {
  const [minutes, setMinutes] = storage.useLocal('minutes', 25);
  const alarm = timer.useInterval(() => {
    ui.notify('Break time!', { sound: true });
  }, minutes * 60 * 1000);

  return ui.Window({ title: 'Pomodoro Timer' }, [
    ui.Text({ size: 'xl' }, alarm.remaining),
    ui.Button({ onClick: alarm.toggle }, alarm.running ? 'Pause' : 'Start'),
    ui.Slider({ value: minutes, onChange: setMinutes, min: 1, max: 60 })
  ]);
}
```

If the app tries to call `net.fetch()` without a network capability token, the kernel throws — the function literally doesn't exist in its sandbox.

## Security Model

Prompt injection is the #1 threat when LLMs generate executable code.

### Three Layers of Defense

**Layer 1: Channel Separation**
- System instructions (kernel prompts) and user data (app content) travel on completely separate channels
- Generated apps never see kernel prompts
- Like kernel space vs. user space — but for prompts

**Layer 2: Static Analysis (No LLM in the Loop)**
- Every generated app goes through AST parsing before execution
- Detect: `eval()`, dynamic imports, capability escalation, encoded payloads
- This is deterministic code analysis — no recursive LLM vulnerability

**Layer 3: Runtime Containment**
- WASM sandbox means even if code is malicious, it's contained
- Capability tokens are cryptographic — cannot be forged
- All inter-app communication goes through a kernel message bus
- Resource limits (CPU time, memory) per app

### Threat Model

| Threat | Mitigation |
|--------|-----------|
| Prompt injection in user input | Sanitize before passing to LLM; separate instruction/data channels |
| Generated code with backdoor | AST static analysis + sandboxing + capability enforcement |
| Capability escalation | Tokens are cryptographic, scoped, kernel-enforced |
| Data exfiltration | Network capability required; all traffic logged |
| Cross-app attacks | Complete isolation; message bus is the only channel |
| Supply chain (malicious registry app) | Content-addressed, community-audited, signed by generating model |

## Tech Stack (Planned)

| Component | Technology | Why |
|-----------|-----------|-----|
| Shell / Window Manager | Tauri or Electron | Cross-platform desktop, web rendering |
| App Sandboxing (Phase 1) | iframe + strict CSP | Fast to prototype, well-understood |
| App Sandboxing (Phase 2) | WebAssembly (Extism/Wasmtime) | Memory-safe, near-native, portable |
| Local LLM | Ollama / llama.cpp | Free, private, fast for simple apps |
| Cloud LLM | Claude, OpenAI, OpenRouter, Groq, Together | Complex app generation (pluggable) |
| Registry Backend | SQLite + Git (content-addressed) | Simple, proven, works offline |
| Inter-Process Comm | postMessage → shared-nothing message bus | Secure by default |
| Static Analysis | tree-sitter + custom rules | Fast AST parsing, language-agnostic |

## Open Questions

These are genuinely unsolved and need community input:

1. **App composition** — Can one generated app call another? What's the stable ABI?
2. **State migration** — When you re-generate an app with a better model, how does data migrate?
3. **Non-determinism** — Same prompt + different model = different app. How do we version this?
4. **Performance** — LLM-generated code is rarely optimal. Auto-profile and suggest regeneration?
5. **Trust bootstrapping** — The kernel can't be LLM-generated (chicken-and-egg). Where's the trust root?
6. **Offline-first** — Should the OS work fully offline with local models, or require cloud?
7. **Multi-language** — Should generated apps be restricted to one language (TS?) or support many?

## Quick Start

### Run from source

```bash
git clone https://github.com/DayZAnder/llm-os.git
cd llm-os
cp .env.example .env      # Edit with your LLM provider settings
npm install
node src/server.js         # Open http://localhost:3000
```

Type a description in the prompt bar. The OS generates, analyzes, and sandboxes your app.

### Download a VM image

Three variants available from [Releases](https://github.com/DayZAnder/llm-os/releases):

| Variant | Size | Includes | Use case |
|---------|------|----------|----------|
| **Server** | ~700MB | Alpine + Docker + Node.js + SSH | Full features, headless |
| **Desktop** | ~1GB | Server + Chromium kiosk browser | Boots into full-screen UI |
| **Micro** | ~50MB | Buildroot + Node.js + Dropbear SSH | Minimal, iframe apps only |

Formats: QCOW2 (Proxmox/KVM), VHDX (Hyper-V), OVA (VirtualBox). Default login: `root` / `llmos`.

> **Licensing:** LLM OS code is MIT. VM images include the Linux kernel (GPLv2) and other open-source components. See [THIRD-PARTY-LICENSES.md](THIRD-PARTY-LICENSES.md) for full details and upstream source links.

### Build VM images from source

```bash
# Alpine server/desktop (4GB disk, full Docker support)
docker build -f build/Dockerfile.iso -t llmos-builder .
docker run --rm --privileged -e VARIANT=server -v $(pwd)/build/output:/output llmos-builder

# Buildroot micro (~50MB, no Docker)
docker build -f build/Dockerfile.buildroot -t llmos-buildroot .
docker run --rm --privileged -v $(pwd)/build/output:/output llmos-buildroot
```

## Roadmap

### Phase 1: Web Prototype ✓
- [x] Browser-based shell with prompt bar and window manager
- [x] iframe sandbox with strict CSP
- [x] LLM gateway (Ollama + Claude API, routed by complexity)
- [x] Static analysis pipeline (regex-based, blocks eval/injection)
- [x] Capability system with user approval dialog
- [x] SDK for generated apps (ui, storage, timer, caps)
- [x] App registry with content addressing and community sync
- [x] Docker container runtime for process apps (NanoClaw support)
- [x] Persistent storage (per-app JSON files, 5MB quota, path traversal protection)

### Phase 2: Bootable Appliance ✓
- [x] Alpine Linux VM image (kernel 6.12 + Docker + Node.js)
- [x] QCOW2 (Proxmox/KVM) and VHDX (Hyper-V) output formats
- [x] Auto-start LLM OS kernel on boot (OpenRC service)
- [x] Networking (DHCP + SSH + serial console)
- [x] First-boot setup (Docker pre-warm)
- [x] Configuration helper (`llmos-config`)
- [x] Kiosk browser mode (desktop variant boots into Chromium kiosk)
- [x] OVA format for VirtualBox
- [x] Automated CI builds (GitHub Actions: tests + VM release pipeline)
- [x] Self-improvement scheduler (automated LLM tasks with guardrails)
- [x] Smaller image via Buildroot (~50MB micro variant)

### Phase 3a: WASM Sandbox + System Configuration
- [ ] Replace iframes with WebAssembly (Wasmtime/Extism)
- [ ] Cryptographic capability tokens (HMAC-SHA256)
- [ ] LLM-driven system configuration from natural language (display, browser, boot, storage policies)
- [ ] System capability gates: `system:display`, `system:browser`, `system:boot`, `system:storage`

### Phase 3b: Unikernel
- [ ] Replace Linux with HermitOS (Rust unikernel)
- [ ] Single address space, no syscalls, just our runtime

### Phase 4: Custom Kernel — Zero Linux
- [ ] Rust `no_std` microkernel
- [ ] UEFI boot → framebuffer → WASM runtime
- [ ] LLM-generated device drivers (experimental)
- [ ] Multi-user support, resource scheduling
- [ ] The OS is the LLM. The LLM is the OS.

### Phase 5: Ephemeral OS — Fresh Instance Every Boot
- [ ] OS Profile spec (`data/profile.yaml`) — user identity, preferences, always-on apps
- [ ] Boot-time generation pipeline — kernel reads profile, generates shell UI + core apps on startup
- [ ] Template caching — hash profile → cache generated output, skip re-generation when unchanged
- [ ] Parallel generation — shell, core apps, and services generated concurrently
- [ ] Immutable/persistent partition split — read-only root (regenerated) + persistent `/data` (user state)
- [ ] Profile sync — export/import profiles, carry your OS identity to any machine
- [ ] Local-first generation — boot completes without network, cloud LLMs optional enhancement
- [ ] **Every boot is a fresh OS. Only the user persists.**

> The goal is full kernel independence. We start with a web prototype because a working demo attracts contributors, and the core innovation — LLM generation + sandboxing + capabilities — is orthogonal to the kernel. Every layer gets replaced as we go. Phase 5 takes this further: the OS itself becomes ephemeral. Only the user's identity and data persist — everything else is regenerated from a profile on every boot.

## Self-Improvement

LLM OS can improve itself through automated background tasks. Enable the scheduler and it will generate new apps, test security, improve popular apps, and check quality — all while you're not using the system.

### Quick Start

```bash
# In .env:
SCHEDULER_ENABLED=true
SCHEDULER_PROVIDER=ollama    # free, local — default
SCHEDULER_DAILY_BUDGET=30    # max LLM calls per day
```

Then open the settings panel (gear icon in the top bar) to enable individual tasks.

### Tasks

| Task | Interval | LLM | Description |
|------|----------|-----|-------------|
| **Generate Apps** | 6h | Yes | Creates diverse apps from 80+ prompt categories |
| **Test Security** | 12h | Yes | Generates adversarial code to find analyzer blind spots |
| **Improve Apps** | 24h | Yes | Rewrites popular apps with better UX |
| **Quality Check** | 48h | No | Re-analyzes registry apps against current rules |

### Guardrails

- **Circuit breaker** — 3 consecutive failures auto-disables the task
- **Dedup check** — skips generation if a similar app already exists in the registry
- **Daily token budget** — configurable max LLM calls/day (default: 30)
- **Ollama-only by default** — uses free local inference, Claude requires explicit opt-in
- **Activity defer** — pauses tasks when you're actively using the OS
- **Max registry size** — stops generating when the registry exceeds 500 apps

### Contributing Generated Apps

Apps generated by the scheduler can be exported (Settings > Export Local Apps) and submitted as a PR to the community registry.

## Claude Code CLI

The server and desktop VM images include an `llmos-claude` command that installs and launches [Claude Code](https://docs.anthropic.com/en/docs/claude-code) on demand. Claude Code is an AI coding assistant that can modify the OS, generate apps, and debug issues from the terminal.

```bash
# 1. Set your API key (or use Claude Max for unlimited)
llmos-config set ANTHROPIC_API_KEY sk-ant-api03-...

# 2. Launch Claude Code (installs on first run)
llmos-claude

# 3. Or pass arguments directly
llmos-claude "generate a weather app and publish it to the registry"
```

Claude Code is **not pre-installed** — the `llmos-claude` wrapper installs it via npm on first use. This keeps the base image clean and respects user choice. Requires an Anthropic API key.

## Contributing

### With Claude Code (fastest way)

```bash
gh repo fork DayZAnder/llm-os --clone
cd llm-os
code .
```

Claude Code reads `CLAUDE.md` automatically — it knows the architecture, the values, and what needs building. Just tell it what to work on:

- *"Build a WASM sandbox to replace the iframe approach"*
- *"Add cryptographic capability tokens using Web Crypto"*
- *"Improve the shell UI — add drag-to-resize windows"*
- *"Audit the security — try to escape the sandbox"*

Or pick a ready-made prompt from [CONTRIBUTING.md](CONTRIBUTING.md) — they include full specs.

When done: `node scripts/values-check.js` then `gh pr create`.

### With any other AI tool

Copy a prompt from [CONTRIBUTING.md](CONTRIBUTING.md) into ChatGPT, Copilot, Cursor, or Gemini. There's a context block to prepend that gives any AI the project values and architecture.

### Without AI

Fork, read the README, run `node src/server.js`, pick an [issue](https://github.com/DayZAnder/llm-os/issues).

### The only rule

Run `node scripts/values-check.js` before you PR. It checks your code against the core values. An AI bot also reviews every PR automatically.

## Prior Art & Inspiration

- [Andrej Karpathy's LLM OS concept](https://www.youtube.com/watch?v=zjkBMFhNj_g) — the analogy that started the conversation
- [AIOS](https://github.com/agiresearch/AIOS) — agent scheduling research
- [E2B](https://e2b.dev/) — Firecracker-based sandboxing for AI code
- [Capability-based security](https://en.wikipedia.org/wiki/Capability-based_security) — the OS security model we're adopting
- [Nix](https://nixos.org/) — content-addressed, reproducible package management
- [WASI](https://wasi.dev/) — WebAssembly System Interface for sandboxed system access
- [OWASP Top 10 for Agentic AI (2026)](https://owasp.org/www-project-top-10-for-large-language-model-applications/) — security threat model

## License

MIT — see [LICENSE](LICENSE).
