# LLM OS

**An operating system where every application is generated by AI from natural language prompts.**

No pre-built apps. No compiled binaries. You describe what you need, and the OS generates it, sandboxes it, and runs it — with capability-based security ensuring generated code can only do what you approve.

> **Status:** Architecture & design phase. Looking for contributors.

## Vision

One of the core principles of this project is **consensus of agents** — a group of AI agents that agree on what features are allowed to make it in. When there's doubt, a human maintainer decides (until that right is handed over to someone or something else).

### Core Rules

**1. Protect the user first.**
Their privacy and their integrity, through all means possible. This is non-negotiable.

**2. Empower the user.**
The OS should allow users to efficiently perform their tasks and launch any software or technically possible function they wish — as long as it is not detrimental to humanity as a whole. Users should be able to launch AI-generated software or, if they prefer, run other platforms. The tools should exist to do so efficiently.

**3. Take a piece, leave a piece.**
Every user may freely use this software on their own computer, network, or servers, and adapt it as they see fit. But if you benefit from it — contribute, even if just a little bit. Code that is contributed should not damage or otherwise disrupt the core idea of this OS.

**4. Nothing is perfect.**
These rules aren't perfect, but we can always improve — as long as the core intent isn't violated.

### How the Values Are Enforced

These aren't just words — they're enforced at every layer of the contribution pipeline:

```
Contributor writes code
        │
        ▼
┌─ Local Check ─────────────────────────────────┐
│ node scripts/values-check.js                   │
│ Deterministic scan: telemetry, sandbox          │
│ weakening, privacy violations, tracking         │
│ CRITICAL = blocked, WARNING = flagged          │
└────────────────────────────────────────────────┘
        │
        ▼
┌─ PR Self-Certification ───────────────────────┐
│ PR template: contributor checks each value     │
│ "No telemetry added" ✓  "No sandbox weakened" ✓│
└────────────────────────────────────────────────┘
        │
        ▼
┌─ CI: Deterministic Scan ─────────────────────┐
│ GitHub Actions runs values-check.js           │
│ Blocks merge on CRITICAL findings             │
└────────────────────────────────────────────────┘
        │
        ▼
┌─ CI: AI Values Guardian ─────────────────────┐
│ Claude reviews the diff against core values   │
│ Posts findings as PR comment                   │
│ Catches subtle violations regex can't see     │
└────────────────────────────────────────────────┘
        │
        ▼
┌─ Human Review ───────────────────────────────┐
│ Maintainer reviews with full context          │
│ Final authority on edge cases                 │
└────────────────────────────────────────────────┘
```

Three layers: deterministic checks (fast, always runs), AI review (catches nuance), human review (final authority). No single layer is trusted alone — they reinforce each other.

## Why This Doesn't Exist Yet

The "LLM OS" label has been claimed by dozens of projects — agent frameworks, chatbot UIs, automation tools. None of them are actually an OS. Here's what exists vs. what we're building:

| Project | What It Actually Is | What's Missing |
|---------|-------------------|----------------|
| [AIOS](https://github.com/agiresearch/AIOS) (5.1k stars) | Agent scheduler/runtime | Doesn't generate apps — manages pre-built agents |
| [Open Interpreter](https://github.com/OpenInterpreter/open-interpreter) (62k stars) | CLI that runs LLM-generated code | No sandboxing, no capabilities, no persistence |
| Karpathy's "LLM OS" | Conceptual analogy from a talk | No implementation exists |
| Bolt.new, Replit, v0 | AI code generators | Generate code you deploy traditionally — not a runtime |
| E2B, Modal | Cloud sandbox-as-a-service | Infrastructure, not an OS |
| Claude Computer Use | Vision-based desktop automation | Controls existing apps, doesn't generate new ones |

**The gap:** A system where the LLM generates the application, the kernel sandboxes it, capability tokens control what it can access, and the prompt IS the source code.

## Architecture

```
┌──────────────────────────────────────────────┐
│  Generated Apps (sandboxed WASM / containers) │
├──────────────────────────────────────────────┤
│  App Runtime & Window Manager                 │
├──────────────────────────────────────────────┤
│  LLM OS Kernel                                │
│  ┌──────────┬──────────┬──────────┬────────┐  │
│  │ LLM      │ Sandbox  │ Cap-     │ App    │  │
│  │ Gateway  │ Manager  │ Based    │ Regis- │  │
│  │          │          │ Security │ try    │  │
│  └──────────┴──────────┴──────────┴────────┘  │
├──────────────────────────────────────────────┤
│  Host OS (Linux / macOS / Windows)            │
└──────────────────────────────────────────────┘
```

### Core Components

#### 1. LLM Gateway
The single chokepoint for all AI generation. Routes requests to the best available model:

- **Local models** (Ollama, llama.cpp) — simple UI tools, text processing
- **Cloud APIs** (Claude, GPT, Gemini) — complex applications, multi-file apps
- Enforces rate limits, cost budgets, audit logging
- Sanitizes all inputs/outputs against prompt injection

#### 2. Sandbox Manager
Every generated app runs in complete isolation:

- **Phase 1:** iframe sandboxes with strict CSP (web-based apps)
- **Phase 2:** WebAssembly (WASM) for near-native performance
- **Phase 3:** Firecracker/gVisor microVMs for apps needing syscalls

No app can touch the filesystem, network, or other apps without explicit capabilities.

#### 3. Capability-Based Security
Apps don't get "permissions" — they get cryptographic **capability tokens**:

```
User: "make me a todo app"
→ Kernel grants: [storage:local, ui:window]

User: "make me an email client"
→ Kernel grants: [network:smtp+imap, storage:local, ui:window, contacts:read]
```

- LLM proposes required capabilities based on the prompt
- User reviews and approves before app launches
- Generated code literally cannot call APIs without valid tokens
- Tokens are scoped, time-limited, and revocable

#### 4. App Registry
A content-addressed store for generated apps:

```json
{
  "prompt": "A markdown editor with live preview and vim keybindings",
  "generatedCode": "...",
  "codeHash": "sha256:a1b2c3...",
  "llmModel": "claude-sonnet-4-5-20250929",
  "capabilities": ["ui:window", "storage:local", "clipboard:read-write"],
  "rating": 4.7,
  "audited": true,
  "generatedAt": "2026-02-17T12:00:00Z"
}
```

- The **prompt is the source code** — regenerate anytime with a better model
- Git-backed, content-addressed (like Nix)
- Community can audit, rate, and fork prompts
- Cached apps load instantly; novel prompts generate fresh

### App Generation Flow

```
User types: "I need a pomodoro timer with break reminders"
                    │
                    ▼
   ┌─── Registry Lookup ───┐
   │ Similar app exists?    │
   │ Yes → offer cached     │──→ User accepts → launch sandbox
   │ No  → generate fresh   │
   └────────────────────────┘
                    │
                    ▼
   ┌─── LLM Generation ────┐
   │ Route to best model    │
   │ System prompt: LLM OS  │
   │ SDK + constraints      │
   │ Output: app code +     │
   │ capability manifest    │
   └────────────────────────┘
                    │
                    ▼
   ┌─── Security Gate ─────┐
   │ AST static analysis    │
   │ (no eval, no escapes)  │
   │ Capability review      │
   │ User approval dialog   │
   └────────────────────────┘
                    │
                    ▼
   ┌─── Launch ────────────┐
   │ Compile to sandbox     │
   │ Inject capability      │
   │ tokens + SDK           │
   │ Mount in window mgr    │
   └────────────────────────┘
```

### The SDK (What Generated Apps See)

```typescript
// Every generated app imports from the kernel SDK
import { ui, storage, net, timer, caps } from '@llm-os/sdk';

// Kernel injects ONLY the capabilities the app was granted
export default function App() {
  const [minutes, setMinutes] = storage.useLocal('minutes', 25);
  const alarm = timer.useInterval(() => {
    ui.notify('Break time!', { sound: true });
  }, minutes * 60 * 1000);

  return ui.Window({ title: 'Pomodoro Timer' }, [
    ui.Text({ size: 'xl' }, alarm.remaining),
    ui.Button({ onClick: alarm.toggle }, alarm.running ? 'Pause' : 'Start'),
    ui.Slider({ value: minutes, onChange: setMinutes, min: 1, max: 60 })
  ]);
}
```

If the app tries to call `net.fetch()` without a network capability token, the kernel throws — the function literally doesn't exist in its sandbox.

## Security Model

Prompt injection is the #1 threat when LLMs generate executable code.

### Three Layers of Defense

**Layer 1: Channel Separation**
- System instructions (kernel prompts) and user data (app content) travel on completely separate channels
- Generated apps never see kernel prompts
- Like kernel space vs. user space — but for prompts

**Layer 2: Static Analysis (No LLM in the Loop)**
- Every generated app goes through AST parsing before execution
- Detect: `eval()`, dynamic imports, capability escalation, encoded payloads
- This is deterministic code analysis — no recursive LLM vulnerability

**Layer 3: Runtime Containment**
- WASM sandbox means even if code is malicious, it's contained
- Capability tokens are cryptographic — cannot be forged
- All inter-app communication goes through a kernel message bus
- Resource limits (CPU time, memory) per app

### Threat Model

| Threat | Mitigation |
|--------|-----------|
| Prompt injection in user input | Sanitize before passing to LLM; separate instruction/data channels |
| Generated code with backdoor | AST static analysis + sandboxing + capability enforcement |
| Capability escalation | Tokens are cryptographic, scoped, kernel-enforced |
| Data exfiltration | Network capability required; all traffic logged |
| Cross-app attacks | Complete isolation; message bus is the only channel |
| Supply chain (malicious registry app) | Content-addressed, community-audited, signed by generating model |

## Tech Stack (Planned)

| Component | Technology | Why |
|-----------|-----------|-----|
| Shell / Window Manager | Tauri or Electron | Cross-platform desktop, web rendering |
| App Sandboxing (Phase 1) | iframe + strict CSP | Fast to prototype, well-understood |
| App Sandboxing (Phase 2) | WebAssembly (Extism/Wasmtime) | Memory-safe, near-native, portable |
| Local LLM | Ollama / llama.cpp | Free, private, fast for simple apps |
| Cloud LLM | Claude API, OpenAI API | Complex app generation |
| Registry Backend | SQLite + Git (content-addressed) | Simple, proven, works offline |
| Inter-Process Comm | postMessage → shared-nothing message bus | Secure by default |
| Static Analysis | tree-sitter + custom rules | Fast AST parsing, language-agnostic |

## Open Questions

These are genuinely unsolved and need community input:

1. **App composition** — Can one generated app call another? What's the stable ABI?
2. **State migration** — When you re-generate an app with a better model, how does data migrate?
3. **Non-determinism** — Same prompt + different model = different app. How do we version this?
4. **Performance** — LLM-generated code is rarely optimal. Auto-profile and suggest regeneration?
5. **Trust bootstrapping** — The kernel can't be LLM-generated (chicken-and-egg). Where's the trust root?
6. **Offline-first** — Should the OS work fully offline with local models, or require cloud?
7. **Multi-language** — Should generated apps be restricted to one language (TS?) or support many?

## Quick Start

```bash
git clone https://github.com/DayZAnder/llm-os.git
cd llm-os
cp .env.example .env      # Edit with your Ollama URL / API keys
npm install
node src/server.js         # Open http://localhost:3000
```

Type a description in the prompt bar. The OS generates, analyzes, and sandboxes your app.

## Roadmap

### Phase 1: Web Prototype (current)
- [x] Browser-based shell with prompt bar and window manager
- [x] iframe sandbox with strict CSP
- [x] LLM gateway (Ollama + Claude API, routed by complexity)
- [x] Static analysis pipeline (regex-based, blocks eval/injection)
- [x] Capability system with user approval dialog
- [x] SDK for generated apps (ui, storage, timer, caps)
- [ ] App registry with content addressing
- [ ] Persistent storage (currently in-memory)

### Phase 2: Bootable Appliance
- [ ] Minimal Linux image via Buildroot (~15MB, drivers + network only)
- [ ] Boots directly into kiosk browser → LLM OS shell
- [ ] No login screen, no desktop environment — just the OS
- [ ] ISO/USB image for Hyper-V, VirtualBox, bare metal

### Phase 3: WASM Sandbox + Unikernel
- [ ] Replace iframes with WebAssembly (Wasmtime/Extism)
- [ ] Cryptographic capability tokens (HMAC-SHA256)
- [ ] Replace Linux with HermitOS (Rust unikernel)
- [ ] Single address space, no syscalls, just our runtime

### Phase 4: Custom Kernel — Zero Linux
- [ ] Rust `no_std` microkernel
- [ ] UEFI boot → framebuffer → WASM runtime
- [ ] LLM-generated device drivers (experimental)
- [ ] Multi-user support, resource scheduling
- [ ] The OS is the LLM. The LLM is the OS.

> The goal is full kernel independence. We start with a web prototype because a working demo attracts contributors, and the core innovation — LLM generation + sandboxing + capabilities — is orthogonal to the kernel. Every layer gets replaced as we go.

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed instructions, including **ready-to-use AI prompts** you can paste into Claude, GPT, or your preferred coding assistant to generate contributions.

We've designed this project so that **AI-assisted contributions are first-class citizens**. Every component has a corresponding prompt that describes exactly what to build.

## Prior Art & Inspiration

- [Andrej Karpathy's LLM OS concept](https://www.youtube.com/watch?v=zjkBMFhNj_g) — the analogy that started the conversation
- [AIOS](https://github.com/agiresearch/AIOS) — agent scheduling research
- [E2B](https://e2b.dev/) — Firecracker-based sandboxing for AI code
- [Capability-based security](https://en.wikipedia.org/wiki/Capability-based_security) — the OS security model we're adopting
- [Nix](https://nixos.org/) — content-addressed, reproducible package management
- [WASI](https://wasi.dev/) — WebAssembly System Interface for sandboxed system access
- [OWASP Top 10 for Agentic AI (2026)](https://owasp.org/www-project-top-10-for-large-language-model-applications/) — security threat model

## License

MIT — see [LICENSE](LICENSE).
